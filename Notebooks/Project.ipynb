{"cells":[{"cell_type":"markdown","metadata":{"id":"OpvCcXE-V9si"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEbjArLAc_IE"},"outputs":[],"source":["!pip install ultralytics\n","!pip install dill"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN4Fw5wYyceT"},"outputs":[],"source":["# Libraries for image processing, machine learning, and utilities\n","\n","import cv2\n","\n","import numpy as np\n","\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from torchvision import models\n","\n","from scipy.spatial import distance\n","\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"id":"IMq2tRHrY8R1"},"source":["# Video Utils"]},{"cell_type":"markdown","metadata":{"id":"bcrdeX8A7m5R"},"source":["These functions read_video and write_video are designed to read frames from a video file and write a list of images (frames) back to a video file, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jasmKFmvkMd"},"outputs":[],"source":["def read_video(path_video):\n","    cap = cv2.VideoCapture(path_video)\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if ret:\n","            frames.append(frame)\n","        else:\n","            break\n","    cap.release()\n","    return frames, fps, original_width, original_height\n","\n","def write_video(imgs, fps, path_out='output.mp4'):\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    h, w, _ = imgs[0].shape\n","    out = cv2.VideoWriter(path_out, fourcc, fps, (w, h))\n","    for img in imgs:\n","        out.write(img)\n","    out.release()"]},{"cell_type":"markdown","metadata":{"id":"262UouyIY5Fh"},"source":["# Ball Detection"]},{"cell_type":"markdown","metadata":{"id":"OXHtetaF0gkc"},"source":["In this part, I'm implementing a ball detection and tracking system for video analysis. The core of the detection model is based on TrackNet, an open-source repository specializing in object tracking. The model architecture and initial implementation are adapted from the TrackNet GitHub repository by [yastrebksv](https://github.com/yastrebksv/TrackNet), with modifications and adjustments made to suit specific project requirements."]},{"cell_type":"markdown","metadata":{"id":"ISCyOupt6v_H"},"source":["This code defines a convolutional block (ConvBlock) in PyTorch using nn.Module. It encapsulates a sequence of operations typically used in convolutional neural networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRouMkK66gvw"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)"]},{"cell_type":"markdown","metadata":{"id":"RizAhbgu7A_q"},"source":["This BallTrackerNet class defines a convolutional neural network (CNN) architecture for ball tracking.\n","\n","The network consists of:\n","\n","* Encoder: Convolutional layers followed by max pooling to downsample the input image.\n","* Bottleneck: Further convolutional layers to capture complex features.\n","* Decoder: Upsampling layers followed by convolutional layers to reconstruct the spatial dimensions.\n","* Output: Final convolutional layer to produce the desired number of output channels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ufdzbkptebm3"},"outputs":[],"source":["import torch.nn as nn\n","\n","class BallTrackerNet(nn.Module):\n","    def __init__(self, input_channels=3, out_channels=14):\n","        super().__init__()\n","        self.out_channels = out_channels\n","        self.input_channels = input_channels\n","\n","        self.conv1 = ConvBlock(in_channels=self.input_channels, out_channels=64)\n","        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n","        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n","        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n","        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n","        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n","        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n","\n","        self.ups1 = nn.Upsample(scale_factor=2)\n","        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n","        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n","        self.conv13 = ConvBlock(in_channels=256, out_channels=256)\n","\n","        self.ups2 = nn.Upsample(scale_factor=2)\n","        self.conv14 = ConvBlock(in_channels=256, out_channels=128)\n","        self.conv15 = ConvBlock(in_channels=128, out_channels=128)\n","\n","        self.ups3 = nn.Upsample(scale_factor=2)\n","        self.conv16 = ConvBlock(in_channels=128, out_channels=64)\n","        self.conv17 = ConvBlock(in_channels=64, out_channels=64)\n","\n","        self.conv18 = ConvBlock(in_channels=64, out_channels=self.out_channels)\n","\n","        self._init_weights()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.pool1(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.pool2(x)\n","        x = self.conv5(x)\n","        x = self.conv6(x)\n","        x = self.conv7(x)\n","        x = self.pool3(x)\n","\n","        x = self.conv8(x)\n","        x = self.conv9(x)\n","        x = self.conv10(x)\n","\n","        x = self.ups1(x)\n","        x = self.conv11(x)\n","        x = self.conv12(x)\n","        x = self.conv13(x)\n","\n","        x = self.ups2(x)\n","        x = self.conv14(x)\n","        x = self.conv15(x)\n","\n","        x = self.ups3(x)\n","        x = self.conv16(x)\n","        x = self.conv17(x)\n","\n","        x = self.conv18(x)\n","        return x\n","\n","    def _init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Conv2d):\n","                nn.init.uniform_(module.weight, -0.05, 0.05)\n","                if module.bias is not None:\n","                    nn.init.constant_(module.bias, 0)\n","\n","            elif isinstance(module, nn.BatchNorm2d):\n","                nn.init.constant_(module.weight, 1)\n","                nn.init.constant_(module.bias, 0)"]},{"cell_type":"markdown","metadata":{"id":"Ca76weaW5vpB"},"source":["The BallDetector class is designed to detect and track a ball in a video using a pre-trained neural network model (BallTrackerNet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0E7nOFbqbo3"},"outputs":[],"source":["class BallDetector:\n","    def __init__(self, path_model=None, device='cuda'):\n","        self.model = BallTrackerNet(input_channels=9, out_channels=256)\n","        self.device = device\n","\n","        if path_model:\n","            self.model.load_state_dict(torch.load(path_model, map_location=device))\n","            self.model = self.model.to(device)\n","            self.model.eval()\n","\n","        self.original_width = None\n","        self.original_height = None\n","        self.resized_width = 640\n","        self.resized_height = 360\n","        self.scale_factor = None\n","        self.scaler = torch.cuda.amp.GradScaler()\n","\n","    def set_scale_factor(self, original_width, original_height):\n","        self.original_width = original_width\n","        self.original_height = original_height\n","        self.scale_factor = self.original_width / self.resized_width\n","\n","    def infer_model(self, frames):\n","        ball_track = [(None, None)] * 2\n","        prev_pred = [None, None]\n","        batch_size = 16\n","\n","        for start in range(2, len(frames), batch_size):\n","            batch_frames = frames[start:start + batch_size]\n","            imgs_batch = []\n","\n","            for num in range(len(batch_frames)):\n","                img = cv2.resize(batch_frames[num], (self.resized_width, self.resized_height))\n","                img_prev = cv2.resize(frames[start + num - 1], (self.resized_width, self.resized_height))\n","                img_preprev = cv2.resize(frames[start + num - 2], (self.resized_width, self.resized_height))\n","                imgs = np.concatenate((img, img_prev, img_preprev), axis=2)\n","                imgs = imgs.astype(np.float32) / 255.0\n","                imgs = np.rollaxis(imgs, 2, 0)\n","                imgs_batch.append(imgs)\n","\n","            inp = np.stack(imgs_batch, axis=0)\n","            inp = torch.from_numpy(inp).half().to(self.device)\n","\n","            with torch.no_grad():\n","                with torch.cuda.amp.autocast():\n","                    out = self.model(inp)\n","\n","            output = out.argmax(dim=1).detach().cpu().numpy()\n","            for num in range(len(output)):\n","                x_pred, y_pred = self.postprocess(output[num], prev_pred)\n","                prev_pred = [x_pred, y_pred]\n","                ball_track.append((x_pred, y_pred))\n","\n","        return ball_track\n","\n","    def postprocess(self, feature_map, prev_pred, max_dist=80):\n","        feature_map *= 255\n","        feature_map = feature_map.reshape((self.resized_height, self.resized_width))\n","        feature_map = feature_map.astype(np.uint8)\n","        ret, heatmap = cv2.threshold(feature_map, 127, 255, cv2.THRESH_BINARY)\n","        circles = cv2.HoughCircles(heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=1, param1=50, param2=2, minRadius=2,\n","                                   maxRadius=7)\n","        x, y = None, None\n","        if circles is not None:\n","            if prev_pred[0] is not None:\n","                for i in range(len(circles[0])):\n","                    x_temp = circles[0][i][0] * self.scale_factor\n","                    y_temp = circles[0][i][1] * self.scale_factor\n","                    dist = distance.euclidean((x_temp, y_temp), prev_pred)\n","                    if dist < max_dist:\n","                        x, y = x_temp, y_temp\n","                        break\n","            else:\n","                x = circles[0][0][0] * self.scale_factor\n","                y = circles[0][0][1] * self.scale_factor\n","        return x, y\n","\n","    def calculate_ball_speed(self, ball_tracks, fps, meters_per_pixel=0.0145):\n","        ball_speeds_kmph = []\n","        for i in range(1, len(ball_tracks)):\n","            if ball_tracks[i][0] is not None and ball_tracks[i-1][0] is not None:\n","                dist_pixels = distance.euclidean(ball_tracks[i], ball_tracks[i-1])\n","                speed_mps = dist_pixels * meters_per_pixel * fps\n","                speed_kmph = speed_mps * 3.6\n","                ball_speeds_kmph.append(speed_kmph)\n","            else:\n","                ball_speeds_kmph.append(None)\n","        return ball_speeds_kmph"]},{"cell_type":"markdown","metadata":{"id":"fDEG_smV6NqH"},"source":["The next function draws the traced ball position on each frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZBOWY-E6K7X"},"outputs":[],"source":["def draw_ball_trace(frames, ball_track):\n","    imgs_res = []\n","    for i in range(len(frames)):\n","        img_res = frames[i].copy()\n","        if ball_track[i][0] is not None:\n","            img_res = cv2.circle(img_res, (int(ball_track[i][0]), int(ball_track[i][1])), radius=5, color=(0, 255, 0), thickness=2)\n","            img_res = cv2.putText(img_res, 'ball', org=(int(ball_track[i][0]) + 8, int(ball_track[i][1]) + 8),\n","                                  fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, thickness=2, color=(0, 255, 0))\n","        imgs_res.append(img_res)\n","    return imgs_res"]},{"cell_type":"markdown","metadata":{"id":"cIEBX3B1ZEuX"},"source":["# Player Detection"]},{"cell_type":"markdown","metadata":{"id":"Ed2MZZ8H9zx3"},"source":["Here, i've implemented a custom player detection model using YOLO v8. The training code and details of this model are available in the notebook \"Training.ipynb\".\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_DCJ8gln5fK4"},"source":["The PlayerDetector class is designed to detect and track players in a video using a YOLO model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vu8hhDG_ZJid"},"outputs":[],"source":["class PlayerDetector:\n","    def __init__(self, path_model, device='cuda'):\n","        self.model = YOLO(path_model, verbose=True).to(device)\n","        self.device = device\n","        self.id_counter = 0\n","        self.players = {}\n","        self.player_colors = {}\n","\n","    def preprocess_frames(self, frames):\n","        batch = []\n","        for frame in frames:\n","            img = cv2.resize(frame, (640, 640)).astype(np.float32) / 255.0\n","            img = np.rollaxis(img, 2, 0)\n","            batch.append(img)\n","        return np.array(batch)\n","\n","    def infer_model(self, frames, original_width, original_height):\n","        batch = self.preprocess_frames(frames)\n","        batch_tensor = torch.from_numpy(batch).float().to(self.device)\n","\n","        with torch.no_grad():\n","            results = self.model(batch_tensor)\n","\n","        player_detections = []\n","        for i, result in enumerate(results):\n","            frame = frames[i]\n","            detections = self.postprocess(result, original_width, original_height)\n","            tracked_players = self.track_players(frame, detections)\n","            player_detections.append(tracked_players)\n","\n","        return player_detections\n","\n","    def postprocess(self, result, original_width, original_height):\n","        boxes = result.boxes.xyxy.cpu().numpy()\n","        scores = result.boxes.conf.cpu().numpy()\n","        classes = result.boxes.cls.cpu().numpy()\n","\n","        scale_x = original_width / 640\n","        scale_y = original_height / 640\n","        scaled_boxes = boxes * [scale_x, scale_y, scale_x, scale_y]\n","\n","        valid_indices = (scores > 0.5) & (classes == 0)\n","        return [(scaled_boxes[i], scores[i], classes[i]) for i in range(len(scores)) if valid_indices[i]]\n","\n","    def track_players(self, frame, detections):\n","        tracked_players = {}\n","        for box, score, _ in detections:\n","            x1, y1, x2, y2 = box\n","            box_center = ((x1 + x2) / 2, (y1 + y2) / 2)\n","\n","            matched_id = None\n","            for player_id, (prev_box, _) in self.players.items():\n","                prev_x1, prev_y1, prev_x2, prev_y2 = prev_box\n","                prev_box_center = ((prev_x1 + prev_x2) / 2, (prev_y1 + prev_y2) / 2)\n","                distance = np.linalg.norm(np.array(box_center) - np.array(prev_box_center))\n","\n","                if distance < 50:\n","                    matched_id = player_id\n","                    break\n","\n","            if matched_id is None:\n","                if len(self.players) >= 2:\n","                    continue\n","\n","                matched_id = self.id_counter\n","                self.id_counter += 1\n","                self.player_colors[matched_id] = (128, 128, 128) if matched_id % 2 == 0 else (147, 20, 255)\n","\n","            tracked_players[matched_id] = (box, score)\n","            self.players[matched_id] = (box, score)\n","\n","        for player_id, (box, score) in tracked_players.items():\n","            x1, y1, x2, y2 = box\n","            color = self.player_colors[player_id]\n","            frame = cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n","            frame = cv2.putText(frame, f'Player {player_id}', (int(x1), int(y1) - 10),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","\n","        return tracked_players"]},{"cell_type":"markdown","metadata":{"id":"D1aXKQ-JuYZ4"},"source":["# Key Points Detection"]},{"cell_type":"markdown","metadata":{"id":"2SnrJqeb29d9"},"source":["Here i'm implementing a video analysis system focused on detecting and tracking keypoints on a tennis court. The detection model I'm using is adapted from a repository by [abdullahtarek](https://github.com/abdullahtarek/tennis_analysis), specifically designed for tennis court analysis and keypoints detection. This model forms the core architecture of my project, customized and adjusted to fit the specific requirements of my application."]},{"cell_type":"markdown","metadata":{"id":"qiJuKf-e42ak"},"source":["The CourtLineDetector class utilizes a pretrained ResNet50 model to detect court lines (keypoints) in images. It loads connections between keypoints from a file, refines keypoints to intersections of detected lines, and can draw these keypoints and connections on images or video frames."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ir_xoZ0_432M"},"outputs":[],"source":["class CourtLineDetector:\n","    def __init__(self, model_path, connections_file):\n","        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, 14*2)\n","        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n","        self.model.eval()\n","        self.transform = transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        self.connections = self.load_connections(connections_file)\n","        self.stabilized_keypoints = None\n","\n","    def load_connections(self, connections_file):\n","        connections = []\n","        with open(connections_file, 'r') as f:\n","            next(f)\n","            for line in f:\n","                start, end, distance = line.strip().split(';')\n","                connections.append((int(start), int(end), float(distance)))\n","        return connections\n","\n","    def detect_keypoints(self, frame):\n","        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        image_tensor = self.transform(image_rgb).unsqueeze(0)\n","        with torch.no_grad():\n","            outputs = self.model(image_tensor)\n","        keypoints = outputs.squeeze().cpu().numpy()\n","        original_h, original_w = frame.shape[:2]\n","        keypoints[::2] *= original_w / 224.0\n","        keypoints[1::2] *= original_h / 224.0\n","        keypoints = self.refine_keypoints(frame, keypoints)\n","        return keypoints\n","\n","    def get_keypoints(self, frame):\n","        if self.stabilized_keypoints is None:\n","            self.stabilized_keypoints = self.detect_keypoints(frame)\n","        return self.stabilized_keypoints\n","\n","    def refine_keypoints(self, frame, keypoints):\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n","        edges = cv2.Canny(thresh, 50, 150, apertureSize=3)\n","        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)\n","\n","        if lines is not None:\n","            lines = lines.squeeze()\n","            intersections = self.find_intersections(lines)\n","            adjusted_keypoints = []\n","            for i in range(0, len(keypoints), 2):\n","                x, y = int(keypoints[i]), int(keypoints[i + 1])\n","                nearest_intersection = self.find_nearest_intersection((x, y), intersections)\n","                adjusted_keypoints.extend(nearest_intersection)\n","            return np.array(adjusted_keypoints)\n","        else:\n","            return keypoints\n","\n","    def find_intersections(self, lines):\n","        intersections = []\n","        for i, line1 in enumerate(lines):\n","            for line2 in lines[i + 1:]:\n","                intersection = self.get_line_intersection(line1, line2)\n","                if intersection is not None:\n","                    intersections.append(intersection)\n","        return intersections\n","\n","    def get_line_intersection(self, line1, line2):\n","        x1, y1, x2, y2 = line1\n","        x3, y3, x4, y4 = line2\n","\n","        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n","        if denominator == 0:\n","            return None\n","\n","        intersect_x = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)) / denominator\n","        intersect_y = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)) / denominator\n","        return int(intersect_x), int(intersect_y)\n","\n","    def find_nearest_intersection(self, point, intersections):\n","        x, y = point\n","        nearest_point = min(intersections, key=lambda p: np.sqrt((p[0] - x)**2 + (p[1] - y)**2))\n","        return nearest_point\n","\n","    def draw_keypoints(self, image, keypoints):\n","        for i in range(0, len(keypoints), 2):\n","            x = int(keypoints[i])\n","            y = int(keypoints[i+1])\n","            cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n","            cv2.putText(image, str(i//2), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n","        return image\n","\n","    def draw_connections(self, image, keypoints):\n","        for start, end, distance in self.connections:\n","            x1, y1 = int(keypoints[start * 2]), int(keypoints[start * 2 + 1])\n","            x2, y2 = int(keypoints[end * 2]), int(keypoints[end * 2 + 1])\n","            cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","            cv2.putText(image, f\"{distance:.2f} m\", ((x1 + x2) // 2, (y1 + y2) // 2),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n","            distance_pixels = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n","            cv2.putText(image, f\"{distance_pixels:.2f} px\", ((x1 + x2) // 2, (y1 + y2) // 2 + 20),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n","        return image\n","\n","    def draw_keypoints_and_connections_on_video(self, video_frames, keypoints_list):\n","        output_video_frames = []\n","        for frame, keypoints in zip(video_frames, keypoints_list):\n","            frame = self.draw_keypoints(frame, keypoints)\n","            frame = self.draw_connections(frame, keypoints)\n","            output_video_frames.append(frame)\n","        return output_video_frames"]},{"cell_type":"markdown","metadata":{"id":"OlCmONtYZM8W"},"source":["# Map"]},{"cell_type":"markdown","metadata":{"id":"GBtBBiBw5MmG"},"source":["The MiniCourt class is designed to create and manage a minimap overlay on a video frame, typically for visualizing keypoints, connections, players, and other elements related to a sports scene."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_hRKZ7tkVqc"},"outputs":[],"source":["class MiniCourt:\n","    def __init__(self, frame):\n","        self.drawing_rectangle_width = 250\n","        self.drawing_rectangle_height = 500\n","        self.padding_court = 20\n","        self.frame = frame\n","        self.minimap = self.create_minimap()\n","        self.players = {}\n","        self.player_colors = {}\n","        self.ball_speed = None\n","\n","    def create_minimap(self):\n","        minimap = np.ones((self.drawing_rectangle_height, self.drawing_rectangle_width, 3), dtype=np.uint8) * 255\n","        return minimap\n","\n","    def draw_keypoints(self, keypoints):\n","        for i in range(0, len(keypoints), 2):\n","            x = int(keypoints[i] * self.drawing_rectangle_width / self.frame.shape[1])\n","            y = int(keypoints[i+1] * self.drawing_rectangle_height / self.frame.shape[0])\n","            cv2.circle(self.minimap, (x, y), 5, (0, 0, 255), -1)\n","        return self.minimap\n","\n","    def draw_connections(self, keypoints, connections):\n","        for start, end, _ in connections:\n","            x1 = int(keypoints[start * 2] * self.drawing_rectangle_width / self.frame.shape[1])\n","            y1 = int(keypoints[start * 2 + 1] * self.drawing_rectangle_height / self.frame.shape[0])\n","            x2 = int(keypoints[end * 2] * self.drawing_rectangle_width / self.frame.shape[1])\n","            y2 = int(keypoints[end * 2 + 1] * self.drawing_rectangle_height / self.frame.shape[0])\n","            cv2.line(self.minimap, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","        return self.minimap\n","\n","    def draw_ball(self, ball_position):\n","        if ball_position is not None and ball_position[0] is not None and ball_position[1] is not None:\n","            x = int(ball_position[0] * self.drawing_rectangle_width / self.frame.shape[1])\n","            y = int(ball_position[1] * self.drawing_rectangle_height / self.frame.shape[0])\n","            cv2.circle(self.minimap, (x, y), 5, (0, 255, 0), -1)\n","        return self.minimap\n","\n","    def draw_players(self, players, player_colors):\n","        for player_id, (x, y) in players.items():\n","            player_color = player_colors[player_id]\n","            player_x = int(x * self.drawing_rectangle_width / self.frame.shape[1])\n","            player_y = int(y * self.drawing_rectangle_height / self.frame.shape[0])\n","            cv2.rectangle(self.minimap, (player_x - 10, player_y - 10), (player_x + 10, player_y + 10), player_color, -1)\n","        return self.minimap\n","\n","    def add_title(self, title):\n","        font = cv2.FONT_HERSHEY_SIMPLEX\n","        font_scale = 1\n","        thickness = 2\n","        text_size = cv2.getTextSize(title, font, font_scale, thickness)[0]\n","        text_width = text_size[0]\n","        text_height = text_size[1]\n","\n","        x = (self.drawing_rectangle_width - text_width) // 2\n","        y = text_height + 10\n","\n","        cv2.putText(self.minimap, title, (x, y), font, font_scale, (0, 0, 0), thickness)\n","\n","    def add_minimap_to_frame(self):\n","        height, width, _ = self.frame.shape\n","        y_start = (height - self.drawing_rectangle_height) // 2\n","        self.frame[y_start:y_start+self.drawing_rectangle_height,\n","                   width-self.drawing_rectangle_width-self.padding_court:width-self.padding_court] = self.minimap\n","        return self.frame\n","\n","    def draw_ball_speed(self, ball_speed_kmph, max_speed_kmph=100):\n","        if ball_speed_kmph is not None:\n","            x_start = self.frame.shape[1] - self.drawing_rectangle_width - self.padding_court\n","            y_start = self.frame.shape[0] - 250\n","\n","            speed_kmph_text = f'Speed (km/h):\\n{ball_speed_kmph:.2f}'\n","            for idx, line in enumerate(speed_kmph_text.split('\\n')):\n","                cv2.putText(self.frame, line, (x_start + 10, y_start + 20 + idx * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n","\n","            bar_width = int((ball_speed_kmph / max_speed_kmph) * 300)\n","            bar_height = 30\n","            bar_x_start = x_start - 60\n","            bar_y_start = y_start + 90\n","\n","            cv2.rectangle(self.frame, (bar_x_start, bar_y_start), (bar_x_start + 300, bar_y_start + bar_height), (200, 200, 200), -1)\n","\n","            red_value = int((ball_speed_kmph / max_speed_kmph) * 255)\n","            green_value = 255 - red_value\n","            bar_color = (0, green_value, red_value)\n","\n","            cv2.rectangle(self.frame, (bar_x_start, bar_y_start), (bar_x_start + bar_width, bar_y_start + bar_height), bar_color, -1)\n","\n","            for i in range(0, max_speed_kmph + 1, 10):\n","                line_x = bar_x_start + int((i / max_speed_kmph) * 300)\n","                cv2.line(self.frame, (line_x, bar_y_start - 5), (line_x, bar_y_start + bar_height + 5), (0, 0, 0), 1)\n","                cv2.putText(self.frame, f'{i}', (line_x - 10, bar_y_start + bar_height + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n","\n","        return self.frame"]},{"cell_type":"markdown","metadata":{"id":"wlsD0VXj9T4d"},"source":["# Detector"]},{"cell_type":"markdown","metadata":{"id":"jJ9crLJV789U"},"source":["This UnifiedDetector class integrates various components (BallDetector, PlayerDetector, CourtLineDetector, and MiniCourt) to process frames, detect objects (balls and players), annotate frames with detections and keypoints, and create a minimap overlay with additional annotations such as player positions and ball speed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zNRrJbUytfE"},"outputs":[],"source":["class UnifiedDetector:\n","    def __init__(self, path_ball_model, path_player_model, path_keypoints_model, path_connections_file, device='cuda'):\n","        self.ball_detector = BallDetector(path_ball_model, device)\n","        self.player_detector = PlayerDetector(path_player_model, device)\n","        self.court_line_detector = CourtLineDetector(path_keypoints_model, path_connections_file)\n","        self.device = device\n","        self.stabilized_keypoints = None\n","        self.minimap = None\n","\n","    def process_frame_others(self, frame):\n","        original_height, original_width = frame.shape[:2]\n","        player_detections = self.player_detector.infer_model([frame], original_width, original_height)[0]\n","        keypoints = self.court_line_detector.get_keypoints(frame)\n","        return player_detections, keypoints\n","\n","    def process_video(self, frames, original_width, original_height, batch_size=16):\n","        self.ball_detector.set_scale_factor(original_width, original_height)\n","        ball_tracks = self.ball_detector.infer_model(frames)\n","\n","        player_tracks = []\n","        court_keypoints = []\n","\n","        for i in range(0, len(frames), batch_size):\n","            batch_frames = frames[i:i + batch_size]\n","            batch_player_detections = self.player_detector.infer_model(batch_frames, original_width, original_height)\n","            for frame, player_detections in zip(batch_frames, batch_player_detections):\n","                keypoints = self.court_line_detector.get_keypoints(frame)\n","                player_tracks.append(player_detections)\n","                court_keypoints.append(keypoints)\n","\n","        return ball_tracks, player_tracks, court_keypoints\n","\n","    def annotate_frames(self, frames, ball_tracks, player_tracks, court_keypoints, fps):\n","        annotated_frames = []\n","        ball_speeds_kmph = self.ball_detector.calculate_ball_speed(ball_tracks, fps)\n","\n","        for frame, ball_position, player_detections, keypoints, speed_kmph in zip(frames, ball_tracks, player_tracks, court_keypoints, ball_speeds_kmph):\n","            frame = self.court_line_detector.draw_keypoints(frame, keypoints)\n","            frame = self.court_line_detector.draw_connections(frame, keypoints)\n","\n","            for player_id, (box, _) in player_detections.items():\n","                x1, y1, x2, y2 = box\n","                color = self.player_detector.player_colors[player_id]\n","                frame = cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n","                frame = cv2.putText(frame, f'Player {player_id}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","\n","            frame = draw_ball_trace([frame], [ball_position])[0]\n","\n","            if self.minimap is None:\n","                minimap = MiniCourt(frame)\n","                minimap.draw_keypoints(keypoints)\n","                minimap.draw_connections(keypoints, self.court_line_detector.connections)\n","                minimap.add_title(\"Minimap\")\n","                self.minimap = minimap.minimap.copy()\n","\n","            minimap_frame = self.minimap.copy()\n","            minimap = MiniCourt(frame)\n","            minimap.minimap = minimap_frame\n","            minimap.draw_ball(ball_position)\n","\n","            player_positions = {}\n","            for player_id, (box, _) in player_detections.items():\n","                x1, y1, x2, y2 = box\n","                player_positions[player_id] = ((x1 + x2) / 2, (y1 + y2) / 2)\n","\n","            minimap.draw_players(player_positions, self.player_detector.player_colors)\n","\n","            if speed_kmph is not None:\n","                minimap.draw_ball_speed(speed_kmph)\n","\n","            frame_with_lines = minimap.add_minimap_to_frame()\n","            annotated_frames.append(frame_with_lines)\n","\n","        return annotated_frames"]},{"cell_type":"markdown","metadata":{"id":"sBntU2MB8GGK"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"isHJiCXz8hUY"},"source":["This script is designed to analyze a tennis video by detecting and tracking the ball and players, annotating the frames with these detections, and saving the annotated video."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34362,"status":"ok","timestamp":1720148569730,"user":{"displayName":"Esteban Castiblanco","userId":"11639960260274182731"},"user_tz":300},"id":"Fjtj23p9z-UT","outputId":"4347b627-2da1-46cf-fc77-18462b5c03c3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["\n","0: 640x640 2 Players, 3.1ms\n","1: 640x640 2 Players, 3.1ms\n","2: 640x640 2 Players, 3.1ms\n","3: 640x640 2 Players, 3.1ms\n","4: 640x640 2 Players, 3.1ms\n","5: 640x640 2 Players, 3.1ms\n","6: 640x640 2 Players, 3.1ms\n","7: 640x640 2 Players, 3.1ms\n","8: 640x640 2 Players, 3.1ms\n","9: 640x640 3 Players, 3.1ms\n","10: 640x640 2 Players, 3.1ms\n","11: 640x640 2 Players, 3.1ms\n","12: 640x640 2 Players, 3.1ms\n","13: 640x640 2 Players, 3.1ms\n","14: 640x640 2 Players, 3.1ms\n","15: 640x640 2 Players, 3.1ms\n","Speed: 0.0ms preprocess, 3.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 3 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 3 Players, 3.0ms\n","11: 640x640 3 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.1ms\n","1: 640x640 2 Players, 3.1ms\n","2: 640x640 2 Players, 3.1ms\n","3: 640x640 2 Players, 3.1ms\n","4: 640x640 2 Players, 3.1ms\n","5: 640x640 2 Players, 3.1ms\n","6: 640x640 2 Players, 3.1ms\n","7: 640x640 2 Players, 3.1ms\n","8: 640x640 2 Players, 3.1ms\n","9: 640x640 2 Players, 3.1ms\n","10: 640x640 2 Players, 3.1ms\n","11: 640x640 2 Players, 3.1ms\n","12: 640x640 2 Players, 3.1ms\n","13: 640x640 2 Players, 3.1ms\n","14: 640x640 2 Players, 3.1ms\n","15: 640x640 2 Players, 3.1ms\n","Speed: 0.0ms preprocess, 3.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 3 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 3.0ms\n","1: 640x640 2 Players, 3.0ms\n","2: 640x640 2 Players, 3.0ms\n","3: 640x640 2 Players, 3.0ms\n","4: 640x640 2 Players, 3.0ms\n","5: 640x640 2 Players, 3.0ms\n","6: 640x640 2 Players, 3.0ms\n","7: 640x640 2 Players, 3.0ms\n","8: 640x640 2 Players, 3.0ms\n","9: 640x640 2 Players, 3.0ms\n","10: 640x640 2 Players, 3.0ms\n","11: 640x640 2 Players, 3.0ms\n","12: 640x640 2 Players, 3.0ms\n","13: 640x640 2 Players, 3.0ms\n","14: 640x640 2 Players, 3.0ms\n","15: 640x640 2 Players, 3.0ms\n","Speed: 0.0ms preprocess, 3.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 Players, 56.2ms\n","1: 640x640 2 Players, 56.2ms\n","Speed: 0.0ms preprocess, 56.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"]}],"source":["path_video = 'Tennis-Video-Analysis/Videos/Inputs/Test_1.mp4'\n","path_ball_model = 'Tennis-Video-Analysis/Models/ball.pt'\n","path_player_model = 'Tennis-Video-Analysis/Models/best.pt'\n","path_keypoints_model = '/Tennis-Video-Analysis/Models/keypoints_model.pth'\n","path_connections_file = 'Tennis-Video-Analysis/Configs/court_connections.txt'\n","path_output_video = 'Tennis-Video-Analysis/Videos/Inputs/Test_output_1.mp4'\n","\n","frames, fps, original_width, original_height = read_video(path_video)\n","\n","unified_detector = UnifiedDetector(path_ball_model, path_player_model, path_keypoints_model, path_connections_file)\n","\n","ball_tracks, player_tracks, court_keypoints = unified_detector.process_video(frames, original_width, original_height)\n","\n","annotated_frames = unified_detector.annotate_frames(frames, ball_tracks, player_tracks, court_keypoints, fps)\n","\n","write_video(annotated_frames, fps, path_output_video)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OpvCcXE-V9si","262UouyIY5Fh","cIEBX3B1ZEuX","D1aXKQ-JuYZ4","OlCmONtYZM8W","IMq2tRHrY8R1"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
